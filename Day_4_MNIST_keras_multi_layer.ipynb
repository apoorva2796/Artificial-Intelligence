{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow  as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the data\n",
    "\n",
    "x_train, x_test = x_train / 255.0 , x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(200, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(100, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(60, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(30, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 60)                6060      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                1830      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                310       \n",
      "=================================================================\n",
      "Total params: 185,300\n",
      "Trainable params: 185,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mymodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.compile(optimizer ='adam',\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "60000/60000 [==============================] - 47s 777us/step - loss: 0.2463 - acc: 0.9254\n",
      "Epoch 2/7\n",
      "60000/60000 [==============================] - 31s 520us/step - loss: 0.0998 - acc: 0.9694\n",
      "Epoch 3/7\n",
      "60000/60000 [==============================] - 47s 775us/step - loss: 0.0702 - acc: 0.9782\n",
      "Epoch 4/7\n",
      "60000/60000 [==============================] - 50s 827us/step - loss: 0.0544 - acc: 0.9834\n",
      "Epoch 5/7\n",
      "60000/60000 [==============================] - 48s 798us/step - loss: 0.0449 - acc: 0.98590s - loss: 0.0449 - acc: 0.985\n",
      "Epoch 6/7\n",
      "60000/60000 [==============================] - 53s 886us/step - loss: 0.0377 - acc: 0.9879\n",
      "Epoch 7/7\n",
      "60000/60000 [==============================] - 51s 843us/step - loss: 0.0317 - acc: 0.9901\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x223d2b57e10>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "mymodel.fit(x_train, y_train, epochs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 164us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07982011046943663, 0.9786]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel1 = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(200, activation=tf.sigmoid),\n",
    "    tf.keras.layers.Dense(100, activation=tf.sigmoid),\n",
    "    tf.keras.layers.Dense(60, activation=tf.sigmoid),\n",
    "    tf.keras.layers.Dense(30, activation=tf.sigmoid),\n",
    "    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 60)                6060      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 30)                1830      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                310       \n",
      "=================================================================\n",
      "Total params: 185,300\n",
      "Trainable params: 185,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mymodel1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel1.compile(optimizer ='adam',\n",
    "    loss = 'sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 45s 742us/step - loss: 0.6993 - acc: 0.8119\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 44s 728us/step - loss: 0.1812 - acc: 0.9517\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 44s 741us/step - loss: 0.1198 - acc: 0.9668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x223cbff8b38>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "mymodel1.fit(x_train, y_train, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 162us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12277333396673203, 0.967]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel1.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method compile in module tensorflow.python.keras.engine.training:\n",
      "\n",
      "compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None, **kwargs) method of tensorflow.python.keras.engine.sequential.Sequential instance\n",
      "    Configures the model for training.\n",
      "    \n",
      "    Arguments:\n",
      "        optimizer: String (name of optimizer) or optimizer instance.\n",
      "            See [optimizers](/optimizers).\n",
      "        loss: String (name of objective function) or objective function.\n",
      "            See [losses](/losses).\n",
      "            If the model has multiple outputs, you can use a different loss\n",
      "            on each output by passing a dictionary or a list of losses.\n",
      "            The loss value that will be minimized by the model\n",
      "            will then be the sum of all individual losses.\n",
      "        metrics: List of metrics to be evaluated by the model\n",
      "            during training and testing.\n",
      "            Typically you will use `metrics=['accuracy']`.\n",
      "            To specify different metrics for different outputs of a\n",
      "            multi-output model, you could also pass a dictionary,\n",
      "            such as `metrics={'output_a': 'accuracy'}`.\n",
      "        loss_weights: Optional list or dictionary specifying scalar\n",
      "            coefficients (Python floats) to weight the loss contributions\n",
      "            of different model outputs.\n",
      "            The loss value that will be minimized by the model\n",
      "            will then be the *weighted sum* of all individual losses,\n",
      "            weighted by the `loss_weights` coefficients.\n",
      "            If a list, it is expected to have a 1:1 mapping\n",
      "            to the model's outputs. If a tensor, it is expected to map\n",
      "            output names (strings) to scalar coefficients.\n",
      "        sample_weight_mode: If you need to do timestep-wise\n",
      "            sample weighting (2D weights), set this to `\"temporal\"`.\n",
      "            `None` defaults to sample-wise weights (1D).\n",
      "            If the model has multiple outputs, you can use a different\n",
      "            `sample_weight_mode` on each output by passing a\n",
      "            dictionary or a list of modes.\n",
      "        weighted_metrics: List of metrics to be evaluated and weighted\n",
      "            by sample_weight or class_weight during training and testing.\n",
      "        target_tensors: By default, Keras will create placeholders for the\n",
      "            model's target, which will be fed with the target data during\n",
      "            training. If instead you would like to use your own\n",
      "            target tensors (in turn, Keras will not expect external\n",
      "            Numpy data for these targets at training time), you\n",
      "            can specify them via the `target_tensors` argument. It can be\n",
      "            a single tensor (for a single-output model), a list of tensors,\n",
      "            or a dict mapping output names to target tensors.\n",
      "        **kwargs: These arguments are passed to `tf.Session.run`.\n",
      "    \n",
      "    Raises:\n",
      "        ValueError: In case of invalid arguments for\n",
      "            `optimizer`, `loss`, `metrics` or `sample_weight_mode`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(mymodel.compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Adam in module tensorflow.python.keras.optimizers object:\n",
      "\n",
      "class Adam(Optimizer)\n",
      " |  Adam optimizer.\n",
      " |  \n",
      " |  Default parameters follow those provided in the original paper.\n",
      " |  \n",
      " |  Arguments:\n",
      " |      lr: float >= 0. Learning rate.\n",
      " |      beta_1: float, 0 < beta < 1. Generally close to 1.\n",
      " |      beta_2: float, 0 < beta < 1. Generally close to 1.\n",
      " |      epsilon: float >= 0. Fuzz factor. If `None`, defaults to `K.epsilon()`.\n",
      " |      decay: float >= 0. Learning rate decay over each update.\n",
      " |      amsgrad: boolean. Whether to apply the AMSGrad variant of this\n",
      " |          algorithm from the paper \"On the Convergence of Adam and\n",
      " |          Beyond\".\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Adam\n",
      " |      Optimizer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  get_config(self)\n",
      " |  \n",
      " |  get_updates(self, loss, params)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from Optimizer:\n",
      " |  \n",
      " |  get_gradients(self, loss, params)\n",
      " |      Returns gradients of `loss` with respect to `params`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          loss: Loss tensor.\n",
      " |          params: List of variables.\n",
      " |      \n",
      " |      Returns:\n",
      " |          List of gradient tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: In case any gradient cannot be computed (e.g. if gradient\n",
      " |            function not implemented).\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current value of the weights of the optimizer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the optimizer, from Numpy arrays.\n",
      " |      \n",
      " |      Should only be called after computing the gradients\n",
      " |      (otherwise the optimizer has no weights).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the optimizer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: in case of incompatible weight shapes.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from Optimizer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from Optimizer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(mymodel.optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
